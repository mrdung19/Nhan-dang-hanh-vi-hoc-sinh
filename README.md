<h1 align="center">NH·∫¨N DI·ªÜN H√ÄNH VI C·ª¶A SINH VI√äN TRONG L·ªöP H·ªåC </h1>

<div align="center">

<p align="center">

<div align="center">
  <img src="https://raw.githubusercontent.com/Wipper0000/Student_behavior_recognition/refs/heads/main/anhimage/logodnu.webp" alt="DaiNam University Logo" width="300"/>
  <img src="https://github.com/Wipper0000/Student_behavior_recognition/raw/main/anhimage/LogoAIoTLab.png" alt="·∫¢nh c·ªßa t√¥i" width="300"/>
</div>


</p>

[![Made by AIoTLab](https://img.shields.io/badge/Made%20by%20AIoTLab-blue?style=for-the-badge)](https://www.facebook.com/DNUAIoTLab)
[![Fit DNU](https://img.shields.io/badge/Fit%20DNU-green?style=for-the-badge)](https://fitdnu.net/)
[![DaiNam University](https://img.shields.io/badge/DaiNam%20University-red?style=for-the-badge)](https://dainam.edu.vn)
</div>

<h2 align="center">S·ª≠ D·ª•ng Yolov8 ƒê·ªÉ Nh·∫≠n Di·ªán H√†nh Vi C·ªßa Sinh Vi√™n</h2>

<p align="left">
  Nh·∫≠n di·ªán h√†nh vi h·ªçc sinh trong l·ªõp h·ªçc s·ª≠ d·ª•ng YOLOv8 l√† ·ª©ng d·ª•ng c√¥ng ngh·ªá AI ƒë·ªÉ ph√°t hi·ªán h√†nh vi nh∆∞ gi∆° tay, s·ª≠ d·ª•ng ƒëi·ªán tho·∫°i. YOLOv8 gi√∫p nh·∫≠n di·ªán ƒë·ªëi t∆∞·ª£ng trong ·∫£nh/video theo th·ªùi gian th·ª±c, h·ªó tr·ª£ gi√°o vi√™n qu·∫£n l√Ω l·ªõp h·ªçc hi·ªáu qu·∫£ h∆°n. C√¥ng ngh·ªá n√†y gi√∫p tƒÉng c∆∞·ªùng s·ª± t∆∞∆°ng t√°c v√† gi√°m s√°t, n√¢ng cao ch·∫•t l∆∞·ª£ng d·∫°y v√† h·ªçc.
  ƒê·ªÅ t√†i n√†y s·ª≠ d·ª•ng model YOLOV8 ƒë·ªÉ nh·∫≠n di·ªán h√†nh vi h·ªçc sinh v·ªõi c√°c h√†nh vi nh∆∞ gi∆° tay, c√∫i ƒë·∫ßu, s·ª≠ d·ª•ng ƒëi·ªán tho·∫°i/m√°y t√≠nh. YOLOV8 n·ªïi ti·∫øng v·ªõi ch·ª©c nƒÉng ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng v√† ph√¢n lo·∫°i c√πng l√∫c theo th·ªùi gian th·ª±c, gi√∫p gi√°o vi√™n qu·∫£n l√Ω l·ªõp h·ªçc hi·ªáu qu·∫£ h∆°n. B·ªçn em ch·ªçn c√¥ng ngh·∫π n√†y ƒë·ªÉ gi√°m s√°t v√† h·ªó tr·ª£ n√¢ng cao nƒÉng su·∫•t c·ªßa qu√° tr√¨nh gi·∫£ng d·∫°y.


</p>

---

## üåü Ki·∫øn tr√∫c h·ªá th·ªëng
<p align="center">
<img src="https://raw.githubusercontent.com/Wipper0000/Student_behavior_recognition/refs/heads/main/anhimage/Flowchart.png" alt="·∫¢nh c·ªßa t√¥i" width="1000"/>

</p>


---


## üõ†Ô∏è C√îNG NGH·ªÜ S·ª¨ D·ª§NG

<div align="left">

- **YOLOv8( model m ho·∫∑c l)**
- **Google Colab**
- **Th∆∞ vi·ªán Ultralytics**
- **Python 3.x.x**
</div>

##  Y√™u c·∫ßu h·ªá th·ªëng

-C√≥ th·ªÉ s·ª≠ d·ª•ng Visual Studio Code n·∫øu m√°y c√≥ GPU ƒë·ªß m·∫°nh
<br>
  ho·∫∑c l√†
<br>
-S·ª≠ d·ª•ng <a href="https://colab.google/" target="_blank">Google Colab</a> h·ªó tr·ª£ cho d√πng mi·ªÖn ph√≠ GPU ƒë·ªÉ train model.

## üöÄ H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t v√† ch·∫°y


## üöÄ H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t v√† ch·∫°y m√¥ h√¨nh YOLOv8

### B∆∞·ªõc 1: Thu th·∫≠p d·ªØ li·ªáu
S·ª≠ d·ª•ng dataset ƒë√£ ƒë∆∞·ª£c g√°n nh√£n s·∫µn t·∫°i m√¥i tr∆∞·ªùng Tr∆∞·ªùng ƒê·∫°i h·ªçc ƒê·∫°i Nam:

[üëâ Link Dataset](https://universe.roboflow.com/ttnt-nyz2m/ai-fxy4m/dataset/2)

### B∆∞·ªõc 2: S·ª≠ d·ª•ng Google Colab ƒë·ªÉ Train m√¥ h√¨nh
Truy c·∫≠p v√†o Google Colab ƒë·ªÉ th·ª±c hi·ªán hu·∫•n luy·ªán m√¥ h√¨nh YOLOv8.

*L∆∞u √Ω: N√™n s·ª≠ d·ª•ng Colab Pro ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh n·∫∑ng h∆°n.*

```python
from google.colab import drive
drive.mount('/content/drive')
```

### B∆∞·ªõc 3: C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
C√†i ƒë·∫∑t th∆∞ vi·ªán v√† Ultralytics b·∫±ng c√¢u l·ªánh sau:

```bash
!pip install ultralytics
```

### B∆∞·ªõc 4: Hu·∫•n luy·ªán m√¥ h√¨nh
S·ª≠ d·ª•ng l·ªánh d∆∞·ªõi ƒë√¢y ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh YOLOv8:

```bash
!python /content/yolov8/train.py \
    --data "/content/drive/MyDrive/BTL_AII/AI.v3-ai.yolov8pytorch/data.yaml" \
    --cfg "/content/yolov8/cfg/training/yolov8.yaml" \
    --weights "/content/SCB-dataset/yolov8/yolov8.pt" \
    --epochs 50 \
    --batch-size 16 \
    --img-size 640 \
    --device 0 \
    --workers 4 \
    --cache-images \
    --name Yolo7_BTL \
    --project "/content/drive/MyDrive/BTL_AII"
```
*L∆∞u √Ω: Ch·ªânh l·∫°i c√°c tham s·ªë batch-size, workers ph√π h·ª£p v·ªõi c·∫•u h√¨nh GPU.*

### B∆∞·ªõc 5: Nh·∫≠n di·ªán h√†nh vi qua video
Download best.pt t·ª´ file weights c·ªßa file k·∫øt qu·∫£ train, r·ªìi t·∫°o file python ƒë·ªÉ
ch·∫°y m√¥ h√¨nh YOLOv8 ƒë·ªÉ nh·∫≠n di·ªán h√†nh vi trong video s·ª≠ d·ª•ng webcam laptop v·ªõi ƒëo·∫°n m√£ sau:

```python

import cv2
import os
import time
from ultralytics import YOLO

# Load your custom YOLOv8 model
model = YOLO('D:/aiot/models/best (2).pt')  # Replace with the correct path to your model

# Create the 'detected_frames' directory if it doesn't exist
output_folder = 'detected_frames/final'
if not os.path.exists(output_folder):
    os.makedirs(output_folder)

# Open webcam (or video stream if needed)
cap = cv2.VideoCapture(0)  # Use 0 for default webcam, or provide the video stream path

if not cap.isOpened():
    print("Error: Could not open webcam.")
    exit()

frame_count = 0  # Counter for naming saved frames
last_save_time = 0  # Th·ªùi ƒëi·ªÉm l∆∞u ·∫£nh cu·ªëi c√πng

while True:
    ret, frame = cap.read()

    if not ret:
        print("Error: Could not read frame from webcam.")
        break

    # Perform object detection on the frame
    results = model(frame)

    # Ki·ªÉm tra n·∫øu c√≥ ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng
    if len(results) > 0:  # ƒê·∫£m b·∫£o results kh√¥ng r·ªóng
        result = results[0]  # L·∫•y ƒë·ªëi t∆∞·ª£ng Results ƒë·∫ßu ti√™n
        if len(result.boxes) > 0:  # Ki·ªÉm tra n·∫øu c√≥ ƒë·ªëi t∆∞·ª£ng ƒë∆∞·ª£c ph√°t hi·ªán
            # Render results (bounding boxes, labels, etc.)
            annotated_frame = result.plot()  # plot() tr·∫£ v·ªÅ frame ƒë√£ ƒë∆∞·ª£c v·∫Ω

            # L∆∞u ·∫£nh n·∫øu ƒë√£ qua 1 gi√¢y k·ªÉ t·ª´ l·∫ßn l∆∞u tr∆∞·ªõc
            current_time = time.time()
            if current_time - last_save_time >= 1:  # Ki·ªÉm tra th·ªùi gian
                frame_filename = os.path.join(output_folder, f'frame_{frame_count:04d}.jpg')
                cv2.imwrite(frame_filename, annotated_frame)  # Save the frame as an image
                print(f"ƒê√£ l∆∞u ·∫£nh: {frame_filename}")
                last_save_time = current_time  # C·∫≠p nh·∫≠t th·ªùi gian l∆∞u cu·ªëi c√πng
                frame_count += 1
        else:
            # N·∫øu kh√¥ng c√≥ ƒë·ªëi t∆∞·ª£ng, d√πng frame g·ªëc ƒë·ªÉ hi·ªÉn th·ªã
            annotated_frame = frame
    else:
        # N·∫øu kh√¥ng c√≥ k·∫øt qu·∫£, d√πng frame g·ªëc
        annotated_frame = frame

    # Display the annotated frame
    cv2.imshow('Live Stream Object Detection', annotated_frame)

    # Exit loop when 'q' is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release video capture and close window
cap.release()
cv2.destroyAllWindows()
```
Sau ƒë√≥ c√°c frame nh·∫≠n di·ªán ƒë∆∞·ª£c b·ªüi m√¥ h√¨nh s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o folder detected_frames


## K·∫øt qu·∫£ train model
Ma tr·∫≠n nh·∫ßm l·∫´n
<p align="center">
<img src="https://github.com/Wipper0000/Student_behavior_recognition/raw/main/anhimage/confusion_matrix.png" alt="·∫¢nh c·ªßa t√¥i" width="1000"/>

</p>
<br>
ƒê·ªô ch√≠nh x√°c trung b√¨nh c·ªßa c√°c nh√£n
<p align="center">
 <p align="center">
 <img src="https://github.com/Wipper0000/Student_behavior_recognition/raw/main/anhimage/model_test.jpg" alt="·∫¢nh c·ªßa t√¥i" width="1700"/>
</p>
</p>

</p>
## ü§ù ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi 3 th√†nh vi√™n:

| H·ªç v√† T√™n                | Vai tr√≤                  |
|--------------------------|--------------------------|
| V√µ Vƒ©nh Th√°i             | Ph√°t tri·ªÉn to√†n b·ªô m√£ ngu·ªìn,ki·ªÉm th·ª≠, tri·ªÉn khai d·ª± √°n, thuy·∫øt tr√¨nh, ƒë·ªÅ xu·∫•t c·∫£i ti·∫øn.|
| L√™ Ng·ªçc H∆∞ng            | Th·ª±c hi·ªán video gi·ªõi thi·ªáu|
| Nguy·ªÖn Ti·∫øn D≈©ng   | Vi·∫øt b√°o c√°o.  |

¬© 2025 NH√ìM 2, CNTT 16-01, TR∆Ø·ªúNG ƒê·∫†I H·ªåC ƒê·∫†I NAM
